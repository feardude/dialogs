# Домашнее задание №5
В данной работе выполнялись задания, связанные с шардированием данных:
1) разработать микросервис диалогов
2) разработать схему хранения диалогов
3) шардировать диалоги

# Схема приложения
Система диалогов реализована в отдельном микросервисе dialogs. В этом домене я выделил следующие сущности:
- **Диалог**. Состоит из единственного поля `String id`. Идентификатор вычисляется в методе `DialogService#toDialogId` и представляет собой конкатенацию user id, участвующих в диалоге, через запятую. При этом user id, являясь int, всегда в порядке возрастания – это позволяет использовать один и тот же диалог обоим пользователям: два разных вызова `POST /dialog/100/200` и `POST /dialog/200/100` вернут один и тот же диалог `Dialog(id='100,200')`
- **Сообщение**. Привязано к диалогу по id, содержит в себе информацию об авторе, времени, уникальный `UUID id`, а также само сообщение.

## Схема данных в Postgres
```sql
create table dialogs (  
    id text not null  
);

create table messages (  
    id        uuid not null default gen_random_uuid(),  
    dialog_id text not null references dialogs (id),  
    author    int  not null,  
    text      text not null,  
    sent_at   timestamp  
);
```

## Шардирование
Диалогов между пользователями может быть достаточно много, а сообщений внутри каждого диалога ещё больше. Таблицы будут активно расти, и мы быстро дойдём до ограничений по хранению на одном сервере.

Для горизонтального масштабирования здесь подойдёт шардирование данных (диалогов и сообщений). Мы сможем добавлять столько дополнительных узлов в БД, сколько потребуется для оптимального хранения.

В качестве инструмента для шардирования будем использовать **Citus**.
### Ключ шардирования
#### Dialog
Начнём с сущности `Dialog`. Единственное поле, являющееся при этом первичным ключом, – `id`. По нему и будет выполнять шардирование:
```sql
alter table dialogs add constraint dialogs_pk primary key (id);  
select create_distributed_table('dialogs', 'id');
```

#### Message
Теперь нужно настроить шардирование для сущности `Message`. Нам нужно держать сообщения из одного диалога на том же самом шарде. Для этого используем `colocation`. В качестве ключа шардирования выберем `dialog_id`. При этом уникальным идентификатором сообщения является `id (uuid)`. Поэтому для правильной настройки шардирования создадим составной primary key:
```sql
alter table messages add constraint messages_pk primary key (dialog_id, id);  
select create_distributed_table('messages', 'dialog_id', colocate_with => 'dialogs');
```

## Проверка
Я сгенерировал 10 000 диалогов (по 10 диалогов для пользователей с id от 1 до 1 000), а также по 100 сообщений в каждом диалоге – всего получилось 100 000 сообщений.

В кластере CItus 4 узла, по 16 шардов на каждом узле, всего 64 шарда.

Для проверки шардирования посмотрим на план запросов по конкретным id диалогов:
```sql
explain analyze select * from messages where dialog_id = '1,2';

Custom Scan (Citus Adaptive)  (cost=0.00..0.00 rows=0 width=0) (actual time=76.805..76.820 rows=100 loops=1)
  Task Count: 1
  Tuple data received from nodes: 6300 bytes
  Tasks Shown: All
  ->  Task
        Tuple data received from node: 6300 bytes
        Node: host=dialogs-worker-3 port=5432 dbname=postgres
        ->  Index Scan using messages_pk_102126 on messages_102126 messages  (cost=0.41..229.86 rows=100 width=68) (actual time=1.484..1.555 rows=100 loops=1)
"              Index Cond: (dialog_id = '1,2'::text)"
            Planning Time: 16.817 ms
            Execution Time: 3.430 ms
Planning Time: 1.922 ms
Execution Time: 77.109 ms




explain analyze select * from messages where dialog_id = '100,110';

Custom Scan (Citus Adaptive)  (cost=0.00..0.00 rows=0 width=0) (actual time=45.182..45.195 rows=100 loops=1)
  Task Count: 1
  Tuple data received from nodes: 6700 bytes
  Tasks Shown: All
  ->  Task
        Tuple data received from node: 6700 bytes
        Node: host=dialogs-worker-2 port=5432 dbname=postgres
        ->  Bitmap Heap Scan on messages_102133 messages  (cost=5.19..235.86 rows=100 width=68) (actual time=1.675..1.753 rows=100 loops=1)
"              Recheck Cond: (dialog_id = '100,110'::text)"
              Heap Blocks: exact=2
              ->  Bitmap Index Scan on messages_pk_102133  (cost=0.00..5.16 rows=100 width=0) (actual time=1.191..1.191 rows=100 loops=1)
"                    Index Cond: (dialog_id = '100,110'::text)"
            Planning Time: 11.884 ms
            Execution Time: 2.587 ms
Planning Time: 0.651 ms
Execution Time: 45.359 ms
```

Каждый запрос по конкретному диалогу выполнялся только на одном узле (`Task Count: 1`), при этом запросы выполнились на разных узлах: 
- для диалога `1,2` – узел `dialogs-worker-3`
- для диалога `100,110` – узел `dialogs-worker-2`

## Ребалансировка/решардинг
Для подготовки к шардингу и ребалансировке предварительно нужно настроить `wal_level` на всех узлах кластера Citus:
```sql
alter system set wal_level = logical;  
select run_command_on_workers('alter system set wal_level = logical');
```
Не забыть перезапустить весь кластер.

Теперь при добавлении новых узлов в кластер Citus можно запустить ребалансировку шардов следующим образом:
```sql  
select citus_rebalance_start();

-- check periodically until {"task_state_counts": {"done": 18}}
select * from citus_rebalance_status();
```

Согласно документации Citus, такая ребалансировка шардов между узлами выполняется без даунтайма – это позволит сохранить функциональность приложения и одновременно в фоновом режиме перебалансировать шарды в кластере.

# Вывод
В данной работе удалось настроить шардирование для данных в подсистеме диалогов с помощью кластера Citus. Это даёт возможность горизонтально масштабировать БД при росте количества данных и поддерживать оптимальное хранение и скорость выполнения запросов.